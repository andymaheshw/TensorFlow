{"cells":[{"cell_type":"markdown","source":["Deep Learning\n=============\n\nAssignment 3 | Tianzi Cai | 2016-06-30\n\n------------\n\nPreviously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n\nThe goal of this assignment is to explore regularization techniques."],"metadata":{"id":"5hIbr52I7Z7U","colab_type":"text"}},{"cell_type":"code","source":["try:\n  import tensorflow as tf\n  print(\"TensorFlow is already installed\")\nexcept ImportError:\n  print(\"Installing TensorFlow\")\n  import subprocess\n  subprocess.check_call([\"/databricks/python/bin/pip\", \"install\", \"https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl\"])\n  print(\"TensorFlow has been installed on this cluster\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# These are all the modules we'll be using later. Make sure you can import them\n# before proceeding further.\nfrom __future__ import print_function\nimport numpy as np\nimport tensorflow as tf\nfrom six.moves import cPickle as pickle"],"metadata":{"id":"apJbCsBHl-2A","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0.0}},"cellView":"both"},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["First reload the data we generated in `1_notmnist.ipynb`."],"metadata":{"id":"jNWGtZaXn-5j","colab_type":"text"}},{"cell_type":"code","source":["pickle_file = 'notMNIST.pickle'\n\nwith open(pickle_file, 'rb') as f:\n  save = pickle.load(f)\n  train_dataset = save['train_dataset']\n  train_labels = save['train_labels']\n  valid_dataset = save['valid_dataset']\n  valid_labels = save['valid_labels']\n  test_dataset = save['test_dataset']\n  test_labels = save['test_labels']\n  del save  # hint to help gc free up memory\n  print('Training set', train_dataset.shape, train_labels.shape)\n  print('Validation set', valid_dataset.shape, valid_labels.shape)\n  print('Test set', test_dataset.shape, test_labels.shape)"],"metadata":{"colab_type":"code","id":"EYRJ4ICW6-da","colab":{"autoexec":{"startup":false,"wait_interval":0.0},"output_extras":[{"item_id":1.0}]},"executionInfo":{"user_tz":420.0,"timestamp":1.444485672507E12,"elapsed":186058.0,"status":"ok","user":{"displayName":"Vincent Vanhoucke","permissionId":"05076109866853157986","photoUrl":"//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg","color":"#1FA15D","isMe":true,"sessionId":"2a0a5e044bb03b66","isAnonymous":false,"userId":"102167687554210253930"}},"cellView":"both","outputId":"0d0f85df-155f-4a89-8e7e-ee32df36ec8d"},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Reformat into a shape that's more adapted to the models we're going to train:\n- data as a flat matrix,\n- labels as float 1-hot encodings."],"metadata":{"id":"cC3p0oEyF8QT","colab_type":"text"}},{"cell_type":"code","source":["image_size = 28\nnum_labels = 10\n\ndef reformat(dataset, labels):\n  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32) # One shape dimension can be -1. In this case, the value is inferred from the length of the array and remaining dimensions.\n  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32) # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n  return dataset, labels\ntrain_dataset, train_labels = reformat(train_dataset, train_labels)\nvalid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\ntest_dataset, test_labels = reformat(test_dataset, test_labels)\nprint('Training set: ', train_dataset.shape, train_labels.shape)\nprint('Validation set: ', valid_dataset.shape, valid_labels.shape)\nprint('Test set: ', test_dataset.shape, test_labels.shape)"],"metadata":{"colab_type":"code","id":"H8CBE-WZ8nmj","colab":{"autoexec":{"startup":false,"wait_interval":0.0},"output_extras":[{"item_id":1.0}]},"executionInfo":{"user_tz":420.0,"timestamp":1.444485672525E12,"elapsed":186055.0,"status":"ok","user":{"displayName":"Vincent Vanhoucke","permissionId":"05076109866853157986","photoUrl":"//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg","color":"#1FA15D","isMe":true,"sessionId":"2a0a5e044bb03b66","isAnonymous":false,"userId":"102167687554210253930"}},"cellView":"both","outputId":"ef6c790c-2513-4b09-962e-27c79390c762"},"outputs":[],"execution_count":7},{"cell_type":"code","source":["def accuracy(predictions, labels):\n  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n          / predictions.shape[0])"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### Problem 1\n\nIntroduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor t using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy."],"metadata":{"id":"4riXK3IoHgx6","colab_type":"text"}},{"cell_type":"markdown","source":["https://github.com/arrisray/ud730/blob/38d01d2073e0b01c35df44488415ef6d0fcd6f91/3_regularization.ipynb"],"metadata":{}},{"cell_type":"code","source":["def logistic_graph(beta):\n  graph = tf.Graph()\n  with graph.as_default():\n    # Input data.\n    tf_train_dataset = tf.placeholder(tf.float32, shape = (None, image_size * image_size))\n    tf_train_labels = tf.placeholder(tf.float32, shape = (None, num_labels))\n    tf_valid_dataset = tf.constant(valid_dataset)\n    tf_test_dataset = tf.constant(test_dataset)\n    \n    # Variables.\n    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels])) # think matmul\n    biases = tf.Variable(tf.zeros([num_labels]))\n    \n    # Training.\n    logits = tf.matmul(tf_train_dataset, weights) + biases\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) # this is the loss function that needs to be optimized\n    reg = beta * tf.nn.l2_loss(weights)\n    loss = loss + reg # update loss\n    \n    # Optimizer.\n    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n    \n    # Prediction.\n    train_prediction = tf.nn.softmax(logits)\n    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n  \n  return {'graph': graph,\n         'tf_train_dataset': tf_train_dataset,\n         'tf_train_labels': tf_train_labels,\n         'tf_valid_dataset': tf_valid_dataset,\n         'tf_test_dataset': tf_test_dataset,\n         'loss': loss,\n         'optimizer': optimizer,\n         'train_prediction': train_prediction,\n         'valid_prediction': valid_prediction,\n         'test_prediction': test_prediction}"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["def nn_graph(beta, use_dropout = False):\n  graph = tf.Graph()\n  with graph.as_default():\n    num_hidden_nodes = 1024\n    \n    # Input data.\n    tf_train_dataset = tf.placeholder(tf.float32, shape = (128, image_size * image_size))\n    tf_train_labels = tf.placeholder(tf.float32, shape = (None, num_labels))\n    tf_valid_dataset = tf.constant(valid_dataset)\n    tf_test_dataset = tf.constant(test_dataset)\n    \n    # Variables.\n    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes])) # think matmul\n    biases_1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n    \n    # Hidden layer.\n    hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1)+biases_1)\n    \n    # Output layer.\n    weights_2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n    biases_2 = tf.Variable(tf.zeros([num_labels]))\n    \n    # Training.\n    logits = 0\n    if (use_dropout):\n      logits = tf.matmul(tf.nn.dropout(hidden_layer, 0.5), weights_2) + biases_2\n    else:\n      logits = tf.matmul(hidden_layer, weights_2) + biases_2\n    \n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) \n    reg = beta * tf.nn.l2_loss(weights_1) + beta * tf.nn.l2_loss(weights_2)\n    loss = loss + reg # update loss\n    \n    # Optimizer.\n    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n    \n    # Prediction.\n    train_prediction = tf.nn.softmax(logits)\n    \n    hidden_valid_prediction = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n    valid_prediction = tf.nn.softmax(tf.matmul(hidden_valid_prediction, weights_2) + biases_2)\n    \n    hidden_test_prediction = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n    test_prediction = tf.nn.softmax(tf.matmul(hidden_test_prediction, weights_2) + biases_2)\n  \n  return {'graph': graph,\n         'tf_train_dataset': tf_train_dataset,\n         'tf_train_labels': tf_train_labels,\n         'tf_valid_dataset': tf_valid_dataset,\n         'tf_test_dataset': tf_test_dataset,\n         'loss': loss,\n         'optimizer': optimizer,\n         'train_prediction': train_prediction,\n         'valid_prediction': valid_prediction,\n         'test_prediction': test_prediction}"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["def run_graph(graph_info, num_steps = 801, batch_size = 128, train_size = 20000):\n  graph = graph_info['graph']\n  tf_train_dataset = graph_info['tf_train_dataset']\n  tf_train_labels = graph_info['tf_train_labels']\n  tf_valid_dataset = graph_info['tf_valid_dataset']\n  tf_test_dataset = graph_info['tf_test_dataset']\n  loss = graph_info['loss']\n  optimizer = graph_info['optimizer']\n  train_prediction = graph_info['train_prediction']\n  valid_prediction = graph_info['valid_prediction']\n  test_prediction = graph_info['test_prediction']\n\n  with tf.Session(graph = graph) as session:\n    tf.initialize_all_variables().run()\n    \n    # Am I making unnecessary copies of data here?\n    if train_size != 20000:\n        tmp = np.random.choice(20000, train_size, False)\n        t_dataset = train_dataset[tmp, :]\n        t_labels = train_labels[tmp, :]\n    else: \n        t_dataset = train_dataset[:, :]\n        t_labels = train_labels[:, :]\n   \n    for step in range(num_steps):\n      \n      # Generate a minibatch.\n      offset = (step * batch_size) % (t_labels.shape[0] - batch_size)\n      batch_data = t_dataset[offset:(offset + batch_size), :]\n      batch_labels = t_labels[offset:(offset + batch_size), :]\n      \n      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n      _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n      \n      if (step % 500 == 0):\n        print('Mini-batch (%d) Loss at step %d: %f' % (batch_size, step, l))\n        print('Mini-batch (%d) Training accuracy: %.1f%%' % (batch_size, accuracy(predictions, batch_labels)))\n        print('Mini-batch (%d) Validation accuracy: %.1f%%' % (batch_size, accuracy(valid_prediction.eval(), valid_labels)))\n        \n    print('Mini-batch (%d) Test accuracy: %.1f%%\\n\\n' % (batch_size, accuracy(test_prediction.eval(), test_labels)))\n    #return accuracy(valid_prediction.eval(), valid_labels)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["logistic_graph_info = logistic_graph(beta = 0.1)\nrun_graph(logistic_graph_info, num_steps = 1001, batch_size = 128)\n\nlogistic_graph_info = logistic_graph(beta = 0.01)\nrun_graph(logistic_graph_info, num_steps = 1001, batch_size = 128)\n\nlogistic_graph_info = logistic_graph(beta = 0.001)\nrun_graph(logistic_graph_info, num_steps = 1001, batch_size = 128)\n\nlogistic_graph_info = logistic_graph(beta = 0.0001)\nrun_graph(logistic_graph_info, num_steps = 1001, batch_size = 128)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["nn_graph_info = nn_graph(beta = 0.1, use_dropout = False)\nrun_graph(nn_graph_info, num_steps = 1001, batch_size = 128, train_size = 20000)\n\nnn_graph_info = nn_graph(beta = 0.01, use_dropout = False)\nrun_graph(nn_graph_info, num_steps = 1001, batch_size = 128, train_size = 20000)\n\nnn_graph_info = nn_graph(beta = 0.001, use_dropout = False)\nrun_graph(nn_graph_info, num_steps = 1001, batch_size = 128, train_size = 20000)\n\nnn_graph_info = nn_graph(beta = 0.0001, use_dropout = False)\nrun_graph(nn_graph_info, num_steps = 1001, batch_size = 128, train_size = 20000)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### Problem 2\n\nLet's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?"],"metadata":{}},{"cell_type":"code","source":["# small training data quickly goes to overfit\n\nlogistic_graph_info = logistic_graph(beta = 0.001)\nrun_graph(logistic_graph_info, num_steps = 1001, batch_size = 128, train_size = 4*128)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["nn_graph_info = nn_graph(beta = 0.001, use_dropout = False)\nrun_graph(nn_graph_info, num_steps = 1001, batch_size = 128, train_size = 4*128)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Problem 3\n\nIntroduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides nn.dropout() for that, but you have to make sure it's only inserted during training.\n\nWhat happens to our extreme overfitting case?"],"metadata":{}},{"cell_type":"code","source":["nn_graph_info = nn_graph(beta = 0.005, use_dropout = True)\nrun_graph(nn_graph_info, num_steps = 1001, batch_size = 128, train_size = 20000)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["nn_graph_info = nn_graph(beta = 0.004, use_dropout = True)\nrun_graph(nn_graph_info, num_steps = 1001, batch_size = 128, train_size = 20000)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["### Problem 4\nTry to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\nOne avenue you can explore is to add multiple layers.\n\nAnother one is to use learning rate decay:\n\n```\nglobal_step = tf.Variable(0)  # count the number of steps taken.\nlearning_rate = tf.train.exponential_decay(0.5, global_step, ...)\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n```"],"metadata":{}},{"cell_type":"code","source":["print (2018*128 % (20000-128) + 128)\nprint (2019*128 % (20000-128))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["def multi_nn_graph(beta):\n  graph = tf.Graph()\n  with graph.as_default():\n    num_hidden_nodes_1 = 1024\n    num_hidden_nodes_2 = 512   \n    \n    # Input data. \n    tf_train_dataset = tf.placeholder(tf.float32, shape = (None, image_size * image_size)) # None introduced to take any shape\n    tf_train_labels = tf.placeholder(tf.float32, shape = (None, num_labels))\n    tf_valid_dataset = tf.constant(valid_dataset)\n    tf_test_dataset = tf.constant(test_dataset)\n    \n    # Variables.\n    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes_1], stddev = 0.05))#np.sqrt(2/20000))) # stddev: sqrt(2/size of previous layer)\n    biases_1 = tf.Variable(tf.zeros([num_hidden_nodes_1]))\n    \n    # Hidden layer.\n    hidden_layer_1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1)+biases_1)\n\n    # Variables.\n    weights_2 = tf.Variable(tf.truncated_normal([num_hidden_nodes_1, num_hidden_nodes_2], stddev = 0.05))#np.sqrt(2/num_hidden_nodes_1)))\n    biases_2 = tf.Variable(tf.zeros([num_hidden_nodes_2]))\n    \n    # Hidden layer.\n    hidden_layer_2 = tf.nn.relu(tf.matmul(tf.nn.dropout(hidden_layer_1, keep_prob = 1, seed = 2016), weights_2)+biases_2) # no dropout\n    \n    # Output layer.\n    weights_3 = tf.Variable(tf.truncated_normal([num_hidden_nodes_2, num_labels], stddev = 0.05))#np.sqrt(2/num_hidden_nodes_2)))\n    biases_3 = tf.Variable(tf.zeros([num_labels]))\n    \n    # Training.\n    logits = tf.matmul(tf.nn.dropout(hidden_layer_2, keep_prob = 1, seed = 2016), weights_3) + biases_3 # no dropout\n    \n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) \n    reg = beta * tf.nn.l2_loss(weights_1) + beta * tf.nn.l2_loss(weights_2) + beta * tf.nn.l2_loss(weights_3)\n    loss = loss + reg # update loss\n    \n    # Optimizer.\n    \n    global_step = tf.Variable(0)  # count the number of steps taken.\n    learning_rate = tf.train.exponential_decay(0.5, global_step, decay_steps = 3000, decay_rate = 0.5)\n    # Passing global_step to minimize() will increment it at each step.\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step) \n    # global_step += 1\n    \n    # Prediction.\n    train_prediction = tf.nn.softmax(logits)\n    \n    hidden1_valid_prediction = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n    hidden2_valid_prediction = tf.nn.relu(tf.matmul(hidden1_valid_prediction, weights_2) + biases_2)\n    valid_prediction = tf.nn.softmax(tf.matmul(hidden2_valid_prediction, weights_3) + biases_3)\n    \n    hidden1_test_prediction = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n    hidden2_test_prediction = tf.nn.relu(tf.matmul(hidden1_test_prediction, weights_2) + biases_2)\n    test_prediction = tf.nn.softmax(tf.matmul(hidden2_test_prediction, weights_3) + biases_3)\n  \n  return {'graph': graph,\n         'tf_train_dataset': tf_train_dataset,\n         'tf_train_labels': tf_train_labels,\n         'tf_valid_dataset': tf_valid_dataset,\n         'tf_test_dataset': tf_test_dataset,\n         'loss': loss,\n         'optimizer': optimizer,\n         'train_prediction': train_prediction,\n         'valid_prediction': valid_prediction,\n         'test_prediction': test_prediction} "],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["def run_graph(graph_info, num_steps = 3001, batch_size = 128):\n  graph = graph_info['graph']\n  tf_train_dataset = graph_info['tf_train_dataset']\n  tf_train_labels = graph_info['tf_train_labels']\n  tf_valid_dataset = graph_info['tf_valid_dataset']\n  tf_test_dataset = graph_info['tf_test_dataset']\n  loss = graph_info['loss']\n  optimizer = graph_info['optimizer']\n  train_prediction = graph_info['train_prediction']\n  valid_prediction = graph_info['valid_prediction']\n  test_prediction = graph_info['test_prediction']\n\n  with tf.Session(graph = graph) as session:\n    tf.initialize_all_variables().run()\n   \n    for step in range(num_steps):\n      \n      # Generate a minibatch.\n      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n      batch_data = train_dataset[offset:(offset + batch_size), :]\n      batch_labels = train_labels[offset:(offset + batch_size), :]\n      \n      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels} # this will feed the placeholders\n      _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n      \n      if (step % 500 == 0):\n        print('Mini-batch (%d) Loss at step %d: %f' % (batch_size, step, l))\n        print('Mini-batch (%d) Training accuracy: %.1f%%' % (batch_size, accuracy(predictions, batch_labels)))\n        print('Mini-batch (%d) Validation accuracy: %.1f%%' % (batch_size, accuracy(valid_prediction.eval(), valid_labels)))\n      \n    print('Final Test accuracy: %.1f%%\\n\\n' % (accuracy(test_prediction.eval(), test_labels)))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["nn_graph_info = multi_nn_graph(beta = 0.0001)\nrun_graph(nn_graph_info, num_steps = 10000)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["def multi_nn_graph(beta):\n  graph = tf.Graph()\n  with graph.as_default():\n    num_hidden_nodes_1 = 1024\n    num_hidden_nodes_2 = 400   \n    \n    # Input data. \n    tf_train_dataset = tf.placeholder(tf.float32, shape = (None, image_size * image_size)) # None introduced to take any shape\n    tf_train_labels = tf.placeholder(tf.float32, shape = (None, num_labels))\n    tf_valid_dataset = tf.constant(valid_dataset)\n    tf_test_dataset = tf.constant(test_dataset)\n    \n    # Variables.\n    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes_1], tf.sqrt(2/num_hidden_nodes_1)))\n    biases_1 = tf.Variable(tf.zeros([num_hidden_nodes_1]))\n    \n    # Hidden layer.\n    hidden_layer_1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1)+biases_1)\n\n    # Variables.\n    weights_2 = tf.Variable(tf.truncated_normal([num_hidden_nodes_1, num_hidden_nodes_2], tf.sqrt(2/num_hidden_nodes_2)))\n    biases_2 = tf.Variable(tf.zeros([num_hidden_nodes_2]))\n    \n    # Hidden layer.\n    hidden_layer_2 = tf.nn.relu(tf.matmul(tf.nn.dropout(hidden_layer_1, keep_prob = 1, seed = 2016), weights_2)+biases_2) # no dropout\n    \n    # Output layer.\n    weights_3 = tf.Variable(tf.truncated_normal([num_hidden_nodes_2, num_labels], tf.sqrt(2/num_labels)))\n    biases_3 = tf.Variable(tf.zeros([num_labels]))\n    \n    # Training.\n    logits = tf.matmul(tf.nn.dropout(hidden_layer_2, keep_prob = 1, seed = 2016), weights_3) + biases_3 # no dropout\n    \n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) \n    reg = beta * tf.nn.l2_loss(weights_1) + beta * tf.nn.l2_loss(weights_2) + beta * tf.nn.l2_loss(weights_3)\n    loss = loss + reg # update loss\n    \n    # Optimizer.\n    \n    global_step = tf.Variable(0)  # count the number of steps taken.\n    learning_rate = tf.train.exponential_decay(0.5, global_step, decay_steps = 1000, decay_rate = 0.5)\n    # Passing global_step to minimize() will increment it at each step.\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step) \n    # global_step += 1\n    \n    # Prediction.\n    train_prediction = tf.nn.softmax(logits)\n    \n    hidden1_valid_prediction = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n    hidden2_valid_prediction = tf.nn.relu(tf.matmul(hidden1_valid_prediction, weights_2) + biases_2)\n    valid_prediction = tf.nn.softmax(tf.matmul(hidden2_valid_prediction, weights_3) + biases_3)\n    \n    hidden1_test_prediction = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n    hidden2_test_prediction = tf.nn.relu(tf.matmul(hidden1_test_prediction, weights_2) + biases_2)\n    test_prediction = tf.nn.softmax(tf.matmul(hidden2_test_prediction, weights_3) + biases_3)\n  \n  return {'graph': graph,\n         'tf_train_dataset': tf_train_dataset,\n         'tf_train_labels': tf_train_labels,\n         'tf_valid_dataset': tf_valid_dataset,\n         'tf_test_dataset': tf_test_dataset,\n         'loss': loss,\n         'optimizer': optimizer,\n         'train_prediction': train_prediction,\n         'valid_prediction': valid_prediction,\n         'test_prediction': test_prediction} "],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["nn_graph_info = multi_nn_graph(beta = 0.0001)\nrun_graph(nn_graph_info, num_steps = 10000)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":29}],"metadata":{"colab":{"name":"1_notmnist.ipynb","default_view":{},"version":"0.3.2","views":{},"provenance":[]},"name":"3_regularization_tz","notebookId":242490},"nbformat":4,"nbformat_minor":0}
