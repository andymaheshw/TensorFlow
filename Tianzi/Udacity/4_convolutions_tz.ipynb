{"cells":[{"cell_type":"markdown","source":["Deep Learning\n=============\n\nAssignment 4 | Tianzi Cai | 2016-07-02\n\n------------\n\nPreviously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n\nThe goal of this assignment is to explore regularization techniques."],"metadata":{"id":"5hIbr52I7Z7U","colab_type":"text"}},{"cell_type":"code","source":["try:\n  import tensorflow as tf\n  print(\"TensorFlow is already installed\")\nexcept ImportError:\n  print(\"Installing TensorFlow\")\n  import subprocess\n  subprocess.check_call([\"/databricks/python/bin/pip\", \"install\", \"https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl\"])\n  print(\"TensorFlow has been installed on this cluster\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# These are all the modules we'll be using later. Make sure you can import them\n# before proceeding further.\nfrom __future__ import print_function\nimport numpy as np\nimport tensorflow as tf\nfrom six.moves import cPickle as pickle"],"metadata":{"id":"apJbCsBHl-2A","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0.0}},"cellView":"both"},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["First reload the data we generated in `1_notmnist.ipynb`."],"metadata":{"id":"jNWGtZaXn-5j","colab_type":"text"}},{"cell_type":"code","source":["pickle_file = 'notMNIST.pickle'\n\nwith open(pickle_file, 'rb') as f:\n  save = pickle.load(f)\n  train_dataset = save['train_dataset']\n  train_labels = save['train_labels']\n  valid_dataset = save['valid_dataset']\n  valid_labels = save['valid_labels']\n  test_dataset = save['test_dataset']\n  test_labels = save['test_labels']\n  del save  # hint to help gc free up memory\n  print('Training set', train_dataset.shape, train_labels.shape)\n  print('Validation set', valid_dataset.shape, valid_labels.shape)\n  print('Test set', test_dataset.shape, test_labels.shape)"],"metadata":{"colab_type":"code","id":"EYRJ4ICW6-da","colab":{"autoexec":{"startup":false,"wait_interval":0.0},"output_extras":[{"item_id":1.0}]},"executionInfo":{"user_tz":420.0,"timestamp":1.444485672507E12,"elapsed":186058.0,"status":"ok","user":{"displayName":"Vincent Vanhoucke","permissionId":"05076109866853157986","photoUrl":"//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg","color":"#1FA15D","isMe":true,"sessionId":"2a0a5e044bb03b66","isAnonymous":false,"userId":"102167687554210253930"}},"cellView":"both","outputId":"0d0f85df-155f-4a89-8e7e-ee32df36ec8d"},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Reformat into a TensorFlow-friendly shape:\n- convolutions need the image data formatted as a cube (width by height by #channels)\n- labels as float 1-hot encodings."],"metadata":{}},{"cell_type":"code","source":["image_size = 28\nnum_labels = 10\nnum_channels = 1 # grayscale\n\nimport numpy as np\n\ndef reformat(dataset, labels):\n  dataset = dataset.reshape(\n    (-1, image_size, image_size, num_channels)).astype(np.float32)\n  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n  return dataset, labels\ntrain_dataset, train_labels = reformat(train_dataset, train_labels)\nvalid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\ntest_dataset, test_labels = reformat(test_dataset, test_labels)\nprint('Training set', train_dataset.shape, train_labels.shape)\nprint('Validation set', valid_dataset.shape, valid_labels.shape)\nprint('Test set', test_dataset.shape, test_labels.shape)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["def accuracy(predictions, labels):\n  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n          / predictions.shape[0])"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."],"metadata":{}},{"cell_type":"code","source":["'''\ntf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\n\nComputes a 2-D convolution given 4-D input and filter tensors.\n\nGiven an input tensor of shape `[batch, in_height, in_width, in_channels]` and a filter / kernel tensor of shape `[filter_height, filter_width, in_channels, out_channels]`, this op performs the following:\n\n1. Flattens the filter to a 2-D matrix with shape `[filter_height * filter_width * in_channels, output_channels]`.\n2. Extracts image patches from the input tensor to form a virtual tensor of shape `[batch, out_height, out_width, filter_height * filter_width * in_channels]`.\n3. For each patch, right-multiplies the filter matrix and the image patch vector.\n\n\n'''"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["batch_size = 16\npatch_size = 5\ndepth = 16\nnum_hidden = 64\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n  # Input data.\n  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels)) #num_channels = 1 is grayscale\n  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n  tf_valid_dataset = tf.constant(valid_dataset)\n  tf_test_dataset = tf.constant(test_dataset)\n  \n  # Variables.\n  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n  layer1_biases = tf.Variable(tf.zeros([depth]))\n  \n  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n  \n  layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1)) # for use in the fully connected layer\n  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n  \n  layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n  \n  # Model.\n  def model(data):\n    conv = tf.nn.conv2d(data, layer1_weights, strides = [1, 2, 2, 1], padding='SAME')\n    hidden = tf.nn.relu(conv + layer1_biases)\n    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n    hidden = tf.nn.relu(conv + layer2_biases)\n    shape = hidden.get_shape().as_list()\n    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n    return tf.matmul(hidden, layer4_weights) + layer4_biases\n  \n  # Training computation.\n  logits = model(tf_train_dataset) # [16,10]\n  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n    \n  # Optimizer.\n  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n  \n  # Predictions for the training, validation, and test data.\n  train_prediction = tf.nn.softmax(logits)\n  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n  test_prediction = tf.nn.softmax(model(tf_test_dataset))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["num_steps = 1001\n\nwith tf.Session(graph=graph) as session:\n  tf.initialize_all_variables().run()\n  print('Initialized')\n  for step in range(num_steps):\n    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n    batch_labels = train_labels[offset:(offset + batch_size), :]\n    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n    _, l, predictions = session.run(\n      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n    if (step % 50 == 0):\n      print('Minibatch loss at step %d: %f' % (step, l))\n      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n      \n  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["batch_size = 16 # equivalent to training 16 feature maps at a time, each of size 28*28\npatch_size = 5 # kernal size of 5*5\ndepth = 16 # equivalent to out_channel for filter in tf.nn.conv2d(), how do we pick out_channel? It should be roughly the same size as the final out_height*out_weight...\nnum_hidden = 64 # num of hidden nodes in the fully connected layer\n\n# Variables.\nlayer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], #[filter_height, filter_width, in_channels, out_channels]\n                                                 stddev=0.1)) # Why is stddev = 0.1?\nlayer1_biases = tf.Variable(tf.zeros([depth]))\n\n# Convolution starts.\nconv = tf.nn.conv2d(train_dataset[0:16,], layer1_weights, strides = [1, 2, 2, 1], padding='SAME') # SAME padding preserves input feature map size if uninary strides\n\n'''\n3 things are happening here:\n1. Flattens the filter or layer1_weights to a 2D matrix with shape [filter_height*filter_width*in_channels, out_channels], or [5*5*1,16] or [25,16]\n2. Exacts image patches from input tensor/feature map to form a virual/temp tensor of shape [batch, out_height, out_weight, filter_height*filter_width*in_channels] or [16,28/2,28/2,5*5*1] or [16,14,14,25]\n3. Multiplies image patch of shape [16,14,14,25] with flattened filter [25,16]\n4. Result (ConvLayer1) should be of shape [16,14,14,16]\n'''\n\nprint ((\"Kernal 1: %s\")%(layer1_weights.get_shape()))\nprint ((\"ConvLayer 1: %s\")%conv.get_shape())\nhidden = tf.nn.relu(conv + layer1_biases) # okay: tensor.shape==[16,14,14,16] + tensor.shape==[16] or tensor.shape==[14,16] or tensor.shape==[14,14,16]\nprint ((\"Biases 1: %s\")%layer1_biases.get_shape())\nprint ((\"Hidden 1: %s\")%hidden.get_shape()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["conv = tf.nn.conv2d(train_dataset[0:16,], layer1_weights, strides = [1, 1, 1, 1], padding='SAME') # [16,28,28,1], [5,5,1,16] = [16,28,28,16]\nhidden = tf.nn.relu(conv + layer1_biases) # [16,28,28,16]\nprint(hidden.get_shape())\npool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') # [16,14,14,16]\nprint(pool.get_shape())"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Variables.\nlayer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], # [filter_height, filter_width, in_channel, out_channel] or [5,5,16,16]\n                                                 stddev=0.1)) # why is stddev = 0.1?\nlayer2_biases = tf.Variable(tf.constant(1.0, shape=[depth])) # to avoid dead neurons\n\n# Convolution Starts.\nconv2 = tf.nn.conv2d(hidden, layer2_weights, strides = [1,2,2,1], padding='SAME') \nhidden2 = tf.nn.relu(conv2 + layer2_biases)\n\n'''\n1. flattens layer2_weights from [5,5,16,16] to [5*5*16,16] or [400,16]\n2. images patches from conv1 [16,14,14,16] to [16,14/2,14/2,5*5*16] or [16,7,7,400]\n3. [16,7,7,400] multiplies [400,16]: [16,7,7,16]\n'''\n\nprint(hidden2.get_shape())"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["conv2 = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME') # [16,14,14,16], [5,5,16,16] = [16,14,14,16]\nhidden2 = tf.nn.relu(conv2 + layer2_biases) # [16,14,14,16]\nprint(hidden2.get_shape())\npool2 = tf.nn.max_pool(hidden2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') #[16,7,7,16]\nprint(pool2.get_shape())"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Variables.\nlayer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], # [28//4*28//4*16,64] or [7,7,64]\n                                                 stddev=0.1)) # for use in the fully connected layer\nlayer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n\nshape = hidden2.get_shape().as_list()\nprint(shape)\nreshape = tf.reshape(hidden2, [shape[0], shape[1] * shape[2] * shape[3]])\nprint(reshape.get_shape())\n\nhidden3 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases) # [16,7*7*16] multiplies [28//4*28//4*16,64], resulting in [16,64]\nprint(hidden3.get_shape())"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], # [28//4*28//4*16,64] or [7,7,64]\n                                                 stddev=0.1)) # for use in the fully connected layer\nlayer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n\nshape = pool2.get_shape().as_list() #[16,7,7,16]\nreshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]]) #[16,784]\nhidden3 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases) #[16,64], this is the fully connected layer\n\ny_conv = tf.nn.softmax(tf.matmul(tf.nn.dropout(hidden3, 0.5), W_fc2) + b_fc2) # implements dropout"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1)) # [64,10]\nlayer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels])) # [10]\n\nlogits = tf.matmul(hidden3, layer4_weights) + layer4_biases # [16,64] * [64,10] + [10], resulting in [16,10]\nprint(logits.get_shape())\n\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\nprint(loss)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Problem 1\n\nThe convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation `(nn.max_pool())` of stride 2 and kernel size 2."],"metadata":{"id":"4riXK3IoHgx6","colab_type":"text"}},{"cell_type":"code","source":["batch_size = 16\npatch_size = 5\ndepth = 16\nnum_hidden = 64\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n  # Input data.\n  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels)) #num_channels = 1 is grayscale\n  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n  tf_valid_dataset = tf.constant(valid_dataset)\n  tf_test_dataset = tf.constant(test_dataset)\n  \n  # Variables.\n  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1)) #[5,5,1,16]\n  layer1_biases = tf.Variable(tf.zeros([depth])) #[16]\n  \n  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1)) #[5,5,16,16]\n  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth])) #[16]\n  \n  layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1)) #[784,64]\n  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden])) #[64]\n  \n  layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1)) #[64,10]\n  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels])) #[64,10]\n  \n  # Model.\n  def model(data):\n    conv = tf.nn.conv2d(data, layer1_weights, strides = [1, 1, 1, 1], padding='SAME') # [16,28,28,1], [5,5,1,16] = [16,28,28,16]\n    hidden = tf.nn.relu(conv + layer1_biases)\n    pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') # [16,14,14,16]\n    \n    conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME') # [16,14,14,16], [5,5,16,16] = [16,14,14,16]\n    hidden = tf.nn.relu(conv + layer2_biases) # [16,14,14,16]\n    pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') #[16,7,7,16]\n    \n    shape = pool.get_shape().as_list() #[16,7,7,16]\n    reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]]) #[16,784]\n    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases) #[16,64]\n    \n    return tf.matmul(hidden, layer4_weights) + layer4_biases #[16,10]\n  \n  # Training computation.\n  logits = model(tf_train_dataset)\n  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n    \n  # Optimizer.\n  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n  \n  # Predictions for the training, validation, and test data.\n  train_prediction = tf.nn.softmax(logits)\n  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n  test_prediction = tf.nn.softmax(model(tf_test_dataset))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["num_steps = 1001\n\nwith tf.Session(graph=graph) as session:\n  tf.initialize_all_variables().run()\n  print('Initialized')\n  for step in range(num_steps):\n    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n    batch_labels = train_labels[offset:(offset + batch_size), :]\n    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n    _, l, predictions = session.run(\n      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n    if (step % 50 == 0):\n      print('Minibatch loss at step %d: %f' % (step, l))\n      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n      \n  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### Problem 2\n\nTry to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay."],"metadata":{}},{"cell_type":"code","source":["batch_size = 16\npatch_size = 5\ndepth = 16\nnum_hidden = 64\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n  # Input data.\n  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels)) #num_channels = 1 is grayscale\n  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n  tf_valid_dataset = tf.constant(valid_dataset)\n  tf_test_dataset = tf.constant(test_dataset)\n  \n  # Variables.\n  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1)) #[5,5,1,16]\n  layer1_biases = tf.Variable(tf.zeros([depth])) #[16]\n  \n  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1)) #[5,5,16,16]\n  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth])) #[16]\n  \n  layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1)) #[784,64]\n  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden])) #[64]\n  \n  layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1)) #[64,10]\n  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels])) #[64,10]\n  \n  # Model.\n  def model(data, dropout = False):\n    if dropout == True: \n      keep_prob = 0.5\n    else:\n      keep_prob = 1\n      \n    conv = tf.nn.conv2d(data, layer1_weights, strides = [1, 1, 1, 1], padding='SAME') # [16,28,28,1], [5,5,1,16] = [16,28,28,16]\n    hidden = tf.nn.relu(conv + layer1_biases)\n    pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') # [16,14,14,16]\n    \n    conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME') # [16,14,14,16], [5,5,16,16] = [16,14,14,16]\n    hidden = tf.nn.relu(conv + layer2_biases) # [16,14,14,16]\n    pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') #[16,7,7,16]\n    \n    shape = pool.get_shape().as_list() #[16,7,7,16]\n    reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]]) #[16,784]\n    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases) #[16,64], this is the fully connected layer\n\n    return tf.matmul(tf.nn.dropout(hidden, keep_prob), layer4_weights) + layer4_biases #[16,10], implements dropout\n  \n  # Training computation.\n  logits = model(tf_train_dataset, dropout = True)\n  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n    \n  # Optimizer.\n  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n  \n#   # Optimizer.\n#   global_step = tf.Variable(0)  # count the number of steps taken.\n#   learning_rate = tf.train.exponential_decay(0.05, global_step, decay_steps = 3000, decay_rate = 0.5)\n#   # Passing global_step to minimize() will increment it at each step.\n#   optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step) \n  \n  # Predictions for the training, validation, and test data.\n  train_prediction = tf.nn.softmax(logits)\n  valid_prediction = tf.nn.softmax(model(tf_valid_dataset, dropout = False)) # shouldn't use dropout for calculating accuracy\n  test_prediction = tf.nn.softmax(model(tf_test_dataset, dropout = False))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["num_steps = 3001\n\nwith tf.Session(graph=graph) as session:\n  tf.initialize_all_variables().run()\n  print('Initialized')\n  for step in range(num_steps):\n    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n    batch_labels = train_labels[offset:(offset + batch_size), :]\n    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n    _, l, predictions = session.run(\n      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n    if (step % 50 == 0):\n      print('Minibatch loss at step %d: %f' % (step, l))\n      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n      \n  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["batch_size = 16\npatch_size = 5\ndepth = 16\nnum_hidden = 64\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n  # Input data.\n  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels)) #num_channels = 1 is grayscale\n  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n  tf_valid_dataset = tf.constant(valid_dataset)\n  tf_test_dataset = tf.constant(test_dataset)\n  \n  # Variables.\n  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1)) #[5,5,1,16]\n  layer1_biases = tf.Variable(tf.zeros([depth])) #[16]\n  \n  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1)) #[5,5,16,16]\n  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth])) #[16]\n  \n  layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1)) #[784,64]\n  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden])) #[64]\n  \n  layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1)) #[64,10]\n  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels])) #[64,10]\n  \n  # Model.\n  def model(data, dropout = False):\n    if dropout == True: \n      keep_prob = 0.5\n    else:\n      keep_prob = 1\n      \n    conv = tf.nn.conv2d(data, layer1_weights, strides = [1, 1, 1, 1], padding='SAME') # [16,28,28,1], [5,5,1,16] = [16,28,28,16]\n    hidden = tf.nn.relu(conv + layer1_biases)\n    pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') # [16,14,14,16]\n    \n    conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME') # [16,14,14,16], [5,5,16,16] = [16,14,14,16]\n    hidden = tf.nn.relu(conv + layer2_biases) # [16,14,14,16]\n    pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') #[16,7,7,16]\n    \n    shape = pool.get_shape().as_list() #[16,7,7,16]\n    reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]]) #[16,784]\n    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases) #[16,64], this is the fully connected layer\n\n    return tf.matmul(tf.nn.dropout(hidden, keep_prob), layer4_weights) + layer4_biases #[16,10], implements dropout\n  \n  # Training computation.\n  logits = model(tf_train_dataset, dropout = True)\n  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n  \n  \n  # Optimizer.\n  global_step = tf.Variable(0)  # count the number of steps taken.\n  learning_rate = tf.train.exponential_decay(0.05, global_step, decay_steps = 3000, decay_rate = 0.5)\n  # Passing global_step to minimize() will increment it at each step.\n  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step) \n  \n  # Predictions for the training, validation, and test data.\n  train_prediction = tf.nn.softmax(logits)\n  valid_prediction = tf.nn.softmax(model(tf_valid_dataset, dropout = False)) # shouldn't use dropout for calculating accuracy\n  test_prediction = tf.nn.softmax(model(tf_test_dataset, dropout = False))"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["num_steps = 3001\n\nwith tf.Session(graph=graph) as session:\n  tf.initialize_all_variables().run()\n  print('Initialized')\n  for step in range(num_steps):\n    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n    batch_labels = train_labels[offset:(offset + batch_size), :]\n    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n    _, l, predictions = session.run(\n      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n    if (step % 50 == 0):\n      print('Minibatch loss at step %d: %f' % (step, l))\n      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n      \n  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["batch_size = 16\npatch_size = 5\ndepth = 16\nnum_hidden1 = 512\nnum_hidden2 = 64\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n  # Input data.\n  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels)) #num_channels = 1 is grayscale\n  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n  tf_valid_dataset = tf.constant(valid_dataset)\n  tf_test_dataset = tf.constant(test_dataset)\n  \n  # Variables.\n  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1)) #[5,5,1,16]\n  layer1_biases = tf.Variable(tf.zeros([depth])) #[16]\n  \n  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1)) #[5,5,16,16]\n  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth])) #[16]\n  \n  layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden1], stddev=0.1)) #[784,128]\n  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden1])) #[128]\n  \n  layer4_weights = tf.Variable(tf.truncated_normal([num_hidden1, num_hidden2], stddev=0.1)) #[128,64]\n  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden2])) #[64]\n  \n  layer5_weights = tf.Variable(tf.truncated_normal([num_hidden2, num_labels], stddev=0.1)) #[64,10]\n  layer5_biases = tf.Variable(tf.constant(1.0, shape=[num_labels])) #[10]\n\n  # Model.\n  def model(data, dropout = False):\n    if dropout == True: \n      keep_prob = 0.8\n    else:\n      keep_prob = 1\n      \n    conv = tf.nn.conv2d(data, layer1_weights, strides = [1, 1, 1, 1], padding='SAME') # [16,28,28,1], [5,5,1,16] = [16,28,28,16]\n    hidden = tf.nn.relu(conv + layer1_biases)\n    pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') # [16,14,14,16], max_pooling\n    \n    conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME') # [16,14,14,16], [5,5,16,16] = [16,14,14,16]\n    hidden = tf.nn.relu(conv + layer2_biases) # [16,14,14,16]\n    pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') #[16,7,7,16]\n    \n    shape = pool.get_shape().as_list() #[16,7,7,16]\n    reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]]) #[16,784]\n    hidden1 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases) #[16,128], this is the first fully connected layer\n\n    hidden2 = tf.matmul(tf.nn.dropout(hidden1, keep_prob), layer4_weights) + layer4_biases # dropout, [16,128],[128,64] \n    \n    return tf.matmul(tf.nn.dropout(hidden2, keep_prob), layer5_weights) + layer5_biases #[16,10], implements dropout\n  \n  # Training computation.\n  logits = model(tf_train_dataset, dropout = True)\n  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n  \n  # Optimizer.\n  global_step = tf.Variable(0)  # count the number of steps taken.\n  learning_rate = tf.train.exponential_decay(0.05, global_step, decay_steps = 3000, decay_rate = 0.5)\n  # Passing global_step to minimize() will increment it at each step.\n  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step) \n  \n  # Predictions for the training, validation, and test data.\n  train_prediction = tf.nn.softmax(logits)\n  valid_prediction = tf.nn.softmax(model(tf_valid_dataset, dropout = False)) # shouldn't use dropout for calculating accuracy\n  test_prediction = tf.nn.softmax(model(tf_test_dataset, dropout = False))"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["num_steps = 3501\n\nwith tf.Session(graph=graph) as session:\n  tf.initialize_all_variables().run()\n  print('Initialized')\n  for step in range(num_steps):\n    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n    batch_labels = train_labels[offset:(offset + batch_size), :]\n    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n    _, l, predictions = session.run(\n      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n    if (step % 50 == 0):\n      print('Minibatch loss at step %d: %f' % (step, l))\n      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n      \n  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["General observations:\n1. Hard to pick learning rate when using learning rate exponential decay\n2. Every time I double `num_hidden`, I get a 0.2% accuracy improvement."],"metadata":{}}],"metadata":{"colab":{"name":"1_notmnist.ipynb","default_view":{},"version":"0.3.2","views":{},"provenance":[]},"name":"4_convolutions_tz","notebookId":247414},"nbformat":4,"nbformat_minor":0}
