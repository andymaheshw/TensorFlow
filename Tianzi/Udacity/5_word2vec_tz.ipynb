{"cells":[{"cell_type":"markdown","source":["Deep Learning\n=============\n\nAssignment 5 | Tianzi Cai | 2016-07-24\n\n------------\n\nThe goal of this assignment is to train a Word2Vec skip-gram model over [Text8](http://mattmahoney.net/dc/textdata) data."],"metadata":{"id":"5hIbr52I7Z7U","colab_type":"text"}},{"cell_type":"code","source":["try:\n  import tensorflow as tf\n  print(\"TensorFlow is already installed\")\nexcept ImportError:\n  print(\"Installing TensorFlow\")\n  import subprocess\n  subprocess.check_call([\"/databricks/python/bin/pip\", \"install\", \"https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl\"])\n  print(\"TensorFlow has been installed on this cluster\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# These are all the modules we'll be using later. Make sure you can import them\n# before proceeding further.\n# %matplotlib inline\nfrom __future__ import print_function\nimport collections\nimport math\nimport numpy as np\nimport os\nimport random\nimport tensorflow as tf\nimport zipfile\n# from matplotlib import pylab\nfrom six.moves import range\nfrom six.moves.urllib.request import urlretrieve\nfrom sklearn.manifold import TSNE"],"metadata":{"id":"apJbCsBHl-2A","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0.0}},"cellView":"both"},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Download the data from the source website if necessary."],"metadata":{"id":"jNWGtZaXn-5j","colab_type":"text"}},{"cell_type":"code","source":["url = 'http://mattmahoney.net/dc/'\n\ndef maybe_download(filename, expected_bytes):\n  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n  if not os.path.exists(filename):\n    filename, _ = urlretrieve(url + filename, filename)\n    '''Return a tuple (filename, headers) where filename is the local file name under which the object can be found, \n    and headers is whatever the info() method of the object returned by urlopen() returned (for a remote object, possibly cached). \n    Exceptions are the same as for urlopen().'''\n  statinfo = os.stat(filename)\n  if statinfo.st_size == expected_bytes:\n    print('Found and verified %s' % filename)\n  else:\n    print(statinfo.st_size)\n    raise Exception(\n      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n  return filename\n\nfilename = maybe_download('text8.zip', 31344016)"],"metadata":{"colab_type":"code","id":"EYRJ4ICW6-da","colab":{"autoexec":{"startup":false,"wait_interval":0.0},"output_extras":[{"item_id":1.0}]},"executionInfo":{"user_tz":420.0,"timestamp":1.444485672507E12,"elapsed":186058.0,"status":"ok","user":{"displayName":"Vincent Vanhoucke","permissionId":"05076109866853157986","photoUrl":"//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg","color":"#1FA15D","isMe":true,"sessionId":"2a0a5e044bb03b66","isAnonymous":false,"userId":"102167687554210253930"}},"cellView":"both","outputId":"0d0f85df-155f-4a89-8e7e-ee32df36ec8d"},"outputs":[],"execution_count":5},{"cell_type":"code","source":["urlretrieve(url + filename, filename)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["os.stat(filename)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Read the data into a string."],"metadata":{}},{"cell_type":"code","source":["def read_data(filename):\n  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n  with zipfile.ZipFile(filename) as f:\n    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n  return data\n  \nwords = read_data(filename)\nprint('Data size %d' % len(words))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["with zipfile.ZipFile(filename) as f:\n  print(f.namelist()) # namelist() Return a list of archive members by name\n  print(f.namelist()[0])\n  print(len(f.namelist()))\n  print(type(f.read(f.namelist()[0])))\n  print(len(f.read(f.namelist()[0])))\n  print(type(tf.compat.as_str(f.read(f.namelist()[0]))))\n  print(len(tf.compat.as_str(f.read(f.namelist()[0]))))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["print(type(words))\nprint(words[:10])"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Build the dictionary and replace rare words with UNK token."],"metadata":{}},{"cell_type":"code","source":["print(collections.Counter(['tianzi', 'tianzi', 'ronny']))\nprint(collections.Counter(['tianzi', 'tianzi', 'ronny', 'ronny', 'y']).most_common(2))\nx = [['UNK', -1]]\nx.extend(collections.Counter(['tianzi', 'tianzi', 'ronny']).most_common(2))\nprint(x)\nx.append(collections.Counter(['t', 't', 'r']).most_common(2))\nx # append() and extend() are destructive operations"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["vocabulary_size = 50000\ncount = [['UNK', -1]]\ncount.extend(collections.Counter(words).most_common(vocabulary_size - 1))\ndictionary = dict()\nfor word, _ in count:\n  dictionary[word] = len(dictionary) # way to add index, which is like the rank, except for 'UNK'\n  \nprint(count[:3])\nprint(dictionary['UNK'])\nprint(dictionary['the'])\nprint(len(dictionary))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["vocabulary_size = 50000\n\ndef build_dataset(words):\n  count = [['UNK', -1]]\n  count.extend(collections.Counter(words).most_common(vocabulary_size - 1)) #https://docs.python.org/dev/library/collections.html#collections.Counter\n  dictionary = dict()\n  for word, _ in count:\n    dictionary[word] = len(dictionary)\n  data = list()\n  unk_count = 0\n  for word in words:\n    if word in dictionary:\n      index = dictionary[word]\n    else:\n      index = 0  # dictionary['UNK']\n      unk_count = unk_count + 1\n    data.append(index)\n  count[0][1] = unk_count\n  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n  return data, count, dictionary, reverse_dictionary\n\ndata, count, dictionary, reverse_dictionary = build_dataset(words)\nprint('Most common words (+UNK)', count[:5])\nprint('Sample data', data[:10])\n# del words  # Hint to reduce memory."],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["{k: dictionary[k] for k in dictionary.keys()[:2]}"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Function to generate a training batch for the skip-gram model."],"metadata":{}},{"cell_type":"code","source":["# print(words[:4])\n# {k: dictionary[k] for k in words[:4]}"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["x = data[:7]\nxbuffer = collections.deque(maxlen = 5)\n'''the deque is bounded to the specified maximum length. Once a bounded length deque is full, when new items are added, a corresponding number of items are discarded from the opposite end'''\nx_index=0\nprint (x)\nfor _ in range(8):\n  xbuffer.append(x[x_index])\n  x_index = (x_index + 1) % len(x)\n  print(xbuffer)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["data_index = 0\nnum_skips = 2\nskip_window = 1\nbatch_size = 8\n                           \nbatch = np.ndarray(shape=(batch_size), dtype=np.int32) # 1D array\nlabels = np.ndarray(shape=(batch_size, 1), dtype=np.int32) # 2D array\nspan = 2 * skip_window + 1 # [ skip_window, target, skip_window ]\nbuffer = collections.deque(maxlen=span) # cool! NEVER used before\n\nfor _ in range(span):\n  buffer.append(data[data_index])\n  data_index = (data_index + 1) % len(data)\n\nprint (buffer)\n\nfor i in range(batch_size // num_skips):\n  target = skip_window\n  targets_to_avoid = [ skip_window ]\n  print (targets_to_avoid)\n  for j in range(num_skips): \n    print('j is %d'%j)\n    while target in targets_to_avoid: \n      target = random.randint(0, span - 1) \n    targets_to_avoid.append(target)\n    print(targets_to_avoid)\n    batch[i * num_skips + j] = buffer[skip_window] \n    print(buffer[skip_window])\n    labels[i * num_skips + j, 0] = buffer[target]\n    print(buffer[target])\n    \n  buffer.append(data[data_index])\n  print(buffer)\n  data_index = (data_index + 1) % len(data)\n  #print(data_index)\n  print('\\n')\n#return batch, labels"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["data_index = 0\n\ndef generate_batch(batch_size, num_skips, skip_window):\n  global data_index\n  assert batch_size % num_skips == 0 # makes sure that batch_size is divisible by num_skips\n  assert num_skips <= 2 * skip_window # pick x out of 2n possible words surrounding target word, x<=2n\n  batch = np.ndarray(shape=(batch_size), dtype=np.int32) # 1D array\n  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32) # 2D array\n  span = 2 * skip_window + 1 # [ skip_window, target, skip_window ]\n  buffer = collections.deque(maxlen=span) # cool! NEVER used before\n  for _ in range(span):\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n    \n  for i in range(batch_size // num_skips): # 8//2 = 4\n    target = skip_window  # first target is right after the first skip_window\n    targets_to_avoid = [ skip_window ] # [1]\n    for j in range(num_skips): # 2\n      while target in targets_to_avoid: # [1]\n        target = random.randint(0, span - 1) # choose from 0 to 3-1\n      targets_to_avoid.append(target) # [1,2]\n      batch[i * num_skips + j] = buffer[skip_window] # buffer[1]\n      labels[i * num_skips + j, 0] = buffer[target] # buffer[2]\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  return batch, labels\n\nprint('data:', [reverse_dictionary[di] for di in data[:8]])\n\nfor num_skips, skip_window in [(2, 1), (4, 2)]:\n    data_index = 0\n    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window) # qhy batch_size = 8?\n    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# okay, this finally makes sense to me. \nprint('data:', [reverse_dictionary[di] for di in data[:16]])\n\nfor num_skips, skip_window in [(2, 1), (4, 2)]:\n    data_index = 0\n    batch, labels = generate_batch(batch_size=16, num_skips=num_skips, skip_window=skip_window) # batch_size = 8?\n    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(16)])"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# okay, this finally makes sense to me. The model is randomly sampling words surrounding it\nprint('data:', [reverse_dictionary[di] for di in data[:16]])\n\nfor num_skips, skip_window in [(2, 1), (3, 2)]:\n    data_index = 0\n    batch, labels = generate_batch(batch_size=12, num_skips=num_skips, skip_window=skip_window)\n    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(12)])"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["batch_size = 128 #  16 <= batch_size <= 512 # https://www.tensorflow.org/versions/r0.7/tutorials/word2vec/index.html#vector-representations-of-words\nembedding_size = 128 # Dimension of the embedding vector.\nskip_window = 1 # How many words to consider left and right.\nnum_skips = 2 # How many times to reuse an input (target) to generate a label. (word)\n# We pick a random validation set to sample nearest neighbors. here we limit the\n# validation samples to the words that have a low numeric ID, which by\n# construction are also the most frequent. (Yes)\nvalid_size = 16 # Random set of words to evaluate similarity on.\nvalid_window = 100 # Only pick dev samples in the head of the distribution.\nvalid_examples = np.array(random.sample(range(valid_window), valid_size)) #huh\nnum_sampled = 64 # Number of negative examples to sample. #huh\n\ngraph = tf.Graph()\n\nwith graph.as_default(), tf.device('/cpu:0'):\n\n  # Input data.\n  train_dataset = tf.placeholder(tf.int32, shape=[batch_size]) # 128\n  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1]) # 128, 1\n  valid_dataset = tf.constant(valid_examples, dtype=tf.int32) # 16\n  \n  # Variables. minval=0, maxval=None\n  embeddings = tf.Variable(\n    tf.random_uniform([vocabulary_size, embedding_size], minval = -1.0, maxval = 1.0)) # \"input\" word vector/embeddings/weight between the input word and the hidden layer\n  softmax_weights = tf.Variable(\n    tf.truncated_normal([vocabulary_size, embedding_size],\n                         stddev=1.0 / math.sqrt(embedding_size))) # 50000, 128\n  softmax_biases = tf.Variable(tf.zeros([vocabulary_size])) # 50000\n  \n  # Model.\n  # Look up embeddings for inputs.\n  embed = tf.nn.embedding_lookup(embeddings, train_dataset) # this acts like indexing lookup because train contains indices\n  '''http://stackoverflow.com/questions/34870614/what-does-tf-nn-embedding-lookup-function-do'''\n  # Compute the softmax loss, using a sample of the negative labels each time.\n  loss = tf.reduce_mean(\n    tf.nn.sampled_softmax_loss(weights = softmax_weights, biases = softmax_biases, inputs = embed,\n                               labels = train_labels, num_sampled = num_sampled, num_classes = vocabulary_size))\n  '''(tz)\n  tf.nn.sampled_softmax_loss() is different from tf.nn.softmax() because sometimes there is a huge number of classes,\n  like this the example above, which has essentially 50000 classes. tf.nn.sampled_softmax_loss() is a faster way to train\n  a softmax classifier over a huge number of classes. it returns the sampled softmax training loss. it is generally an \n  underestimate of the full softmax loss. it is for training only. \n  '''\n\n  # Optimizer.\n  # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n  # This is because the embeddings are defined as a variable quantity and the\n  # optimizer's `minimize` method will by default modify all variable quantities \n  # that contribute to the tensor it is passed. (tz: wow! cool!)\n  # See docs on `tf.train.Optimizer.minimize()` for more details.\n  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n  \n  # Compute the similarity between minibatch examples and all embeddings.\n  # We use the cosine distance:\n  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True)) # 50000, 128\n  normalized_embeddings = embeddings / norm # 50000, 128\n  valid_embeddings = tf.nn.embedding_lookup(\n    normalized_embeddings, valid_dataset) # 16, 128\n  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings)) # 16 by 50000"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["num_steps = 100001\n\nwith tf.Session(graph=graph) as session:\n  tf.initialize_all_variables().run()\n  print('Initialized')\n  average_loss = 0\n  for step in range(num_steps):\n    batch_data, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n    average_loss += l\n    if step % 2000 == 0:\n      if step > 0:\n        average_loss = average_loss / 2000\n      # The average loss is an estimate of the loss over the last 2000 batches.\n      print('Average loss at step %d: %f' % (step, average_loss))\n      average_loss = 0\n    # note that this is expensive (~20% slowdown if computed every 500 steps)\n    if step % 10000 == 0:\n      sim = similarity.eval()\n      for i in range(valid_size):\n        valid_word = reverse_dictionary[valid_examples[i]]\n        top_k = 8 # number of nearest neighbors\n        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n        log = 'Nearest to %s:' % valid_word\n        for k in range(top_k):\n          close_word = reverse_dictionary[nearest[k]]\n          log = '%s %s,' % (log, close_word)\n        print(log)\n  final_embeddings = normalized_embeddings.eval()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["print(batch_data)\nprint(batch_labels)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["num_points = 400\n\ntsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\ntwo_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :])"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["import pandas as pd\ndf = (sqlContext.createDataFrame(pd.DataFrame(two_d_embeddings))) # 400\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["valid_examples"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["final_embeddings"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["print(similarity.get_shape())\nx = similarity[3, :]\nprint(x)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["An alternative to skip-gram is another Word2Vec model called [CBOW](http://arxiv.org/abs/1301.3781) (Continuous Bag of Words). In the CBOW model, instead of predicting a context word from a word vector, you predict a word from the sum of all the word vectors in its context. Implement and evaluate a CBOW model trained on the text8 dataset."],"metadata":{}},{"cell_type":"code","source":["'''https://gist.github.com/discorev/b6a0900a52b62cd04f33'''\n\nimport numpy as np\n\ndata_index = 0\n\ndef generate_batch_cbow(batch_size, skip_window):\n  global data_index\n  context_window = 2 * skip_window # calculate the context_window - this is the total number of words around the target\n  assert batch_size % context_window == 0 # ensure the context window can be taken from the batch size\n  num_labels = batch_size / context_window # the number of labels is the how many context windows fit in the batch\n  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n  labels = np.ndarray(shape=(num_labels, 1), dtype=np.int32)\n  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n  buffer = collections.deque(maxlen=span)\n  for _ in range(span):\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  for i in range(num_labels):\n    target = skip_window  # target label at the center of the buffer\n    labels[i, 0] = buffer[target] # set the label\n    targets_to_avoid = [ skip_window ]\n    for j in range(context_window):\n      while target in targets_to_avoid:\n        target = random.randint(0, span - 1)\n      targets_to_avoid.append(target)\n      batch[i * context_window + j] = buffer[target]\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  return batch, labels\n\nskip_window = 1\nbatch, labels = generate_batch_cbow(8, skip_window)\nprint(np.shape(labels))\nfor i in range(8):\n  print(batch[i], '->', labels[i/(2*skip_window), 0])\n  print(reverse_dictionary[batch[i]], '->', reverse_dictionary[labels[i/(2*skip_window), 0]])\ndel skip_window # remove skip_window setting used for testing"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["batch_size = 128\nembedding_size = 128 # Dimension of the embedding vector.\nskip_window = 1 # How many words to consider left and right.\n# We pick a random validation set to sample nearest neighbors. here we limit the\n# validation samples to the words that have a low numeric ID, which by\n# construction are also the most frequent. \nvalid_size = 16 # Random set of words to evaluate similarity on.\nvalid_window = 100 # Only pick dev samples in the head of the distribution.\nvalid_examples = np.array(random.sample(xrange(valid_window), valid_size))\nnum_sampled = 32 # Number of negative examples to sample.\n\n## General defines\ncontext_window = 2 * skip_window\nnum_labels = batch_size / context_window\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n  # Input data.\n  train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n  train_labels = tf.placeholder(tf.int32, shape=[num_labels, 1])\n  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n  \n  # Variables.\n  embeddings = tf.Variable(\n    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n  softmax_weights = tf.Variable(\n    tf.truncated_normal([vocabulary_size, embedding_size],\n                         stddev=1.0 / math.sqrt(embedding_size)))\n  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n  \n  # Model.\n  # Look up embeddings for inputs.\n  embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n\n  # seq_ids only needs to be generated once so do this as a numpy array rather than a tensor.\n  seq_ids = np.zeros(batch_size, dtype=np.int32)\n  cur_id = -1\n  for i in range(batch_size):\n    if i % context_window == 0:\n      cur_id = cur_id + 1\n    seq_ids[i] = cur_id\n  print(seq_ids)\n  \n  # use segment_sum to add together the related words and reduce the output to be num_labels in size.\n  final_embed = tf.segment_sum(embed, seq_ids)\n  '''https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#segment_sum'''\n  \n  # Compute the softmax loss, using a sample of the negative labels each time.\n  loss = tf.reduce_mean(\n    tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, final_embed,\n                               train_labels, num_sampled, vocabulary_size))\n\n  # Optimizer.\n  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n  \n  # Compute the similarity between minibatch examples and all embeddings.\n  # We use the cosine distance:\n  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n  normalized_embeddings = embeddings / norm\n  valid_embeddings = tf.nn.embedding_lookup(\n    normalized_embeddings, valid_dataset)\n  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["num_steps = 100001\n\nwith tf.Session(graph=graph) as session:\n  tf.initialize_all_variables().run()\n  print(\"Initialized\")\n  average_loss = 0\n  for step in xrange(num_steps):\n    batch_data, batch_labels = generate_batch_cbow(batch_size, skip_window)\n    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n    average_loss += l\n    if step % 2000 == 0:\n      if step > 0:\n        average_loss = average_loss / 2000\n      # The average loss is an estimate of the loss over the last 2000 batches.\n      print(\"Average loss at step\", step, \":\", average_loss)\n      average_loss = 0\n    # note that this is expensive (~20% slowdown if computed every 500 steps)\n    if step % 10000 == 0:\n      sim = similarity.eval()\n      for i in xrange(valid_size):\n        valid_word = reverse_dictionary[valid_examples[i]]\n        top_k = 8 # number of nearest neighbors\n        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n        log = \"Nearest to %s:\" % valid_word\n        for k in xrange(top_k):\n          close_word = reverse_dictionary[nearest[k]]\n          log = \"%s %s,\" % (log, close_word)\n        print(log)\n  final_embeddings = normalized_embeddings.eval()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["batch_size = 128\nembedding_size = 128 # Dimension of the embedding vector.\nskip_window = 1 # How many words to consider left and right.\n# We pick a random validation set to sample nearest neighbors. here we limit the\n# validation samples to the words that have a low numeric ID, which by\n# construction are also the most frequent. \nvalid_size = 16 # Random set of words to evaluate similarity on.\nvalid_window = 100 # Only pick dev samples in the head of the distribution.\nvalid_examples = np.array(random.sample(xrange(valid_window), valid_size))\nnum_sampled = 32 # Number of negative examples to sample.\n\n## General defines\ncontext_window = 2 * skip_window\nnum_labels = batch_size / context_window\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n  # Input data.\n  train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n  train_labels = tf.placeholder(tf.int32, shape=[num_labels, 1])\n  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n  \n  # Variables.\n  embeddings = tf.Variable(\n    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n  softmax_weights = tf.Variable(\n    tf.truncated_normal([vocabulary_size, embedding_size],\n                         stddev=1.0 / math.sqrt(embedding_size)))\n  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n  \n  # Model.\n  # Look up embeddings for inputs.\n  embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n\n  # seq_ids only needs to be generated once so do this as a numpy array rather than a tensor.\n  seq_ids = np.zeros(batch_size, dtype=np.int32)\n  cur_id = -1\n  for i in range(batch_size):\n    if i % context_window == 0:\n      cur_id = cur_id + 1\n    seq_ids[i] = cur_id\n  print(seq_ids)\n  \n  # use segment_mean to add together the related words and reduce the output to be num_labels in size.\n  final_embed = tf.segment_mean(embed, seq_ids)\n  '''https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#segment_sum'''\n  \n  # Compute the softmax loss, using a sample of the negative labels each time.\n  loss = tf.reduce_mean(\n    tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, final_embed,\n                               train_labels, num_sampled, vocabulary_size))\n\n  # Optimizer.\n  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n  \n  # Compute the similarity between minibatch examples and all embeddings.\n  # We use the cosine distance:\n  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n  normalized_embeddings = embeddings / norm\n  valid_embeddings = tf.nn.embedding_lookup(\n    normalized_embeddings, valid_dataset)\n  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["num_steps = 100001\n\nwith tf.Session(graph=graph) as session:\n  tf.initialize_all_variables().run()\n  print(\"Initialized\")\n  average_loss = 0\n  for step in xrange(num_steps):\n    batch_data, batch_labels = generate_batch_cbow(batch_size, skip_window)\n    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n    average_loss += l\n    if step % 2000 == 0:\n      if step > 0:\n        average_loss = average_loss / 2000\n      # The average loss is an estimate of the loss over the last 2000 batches.\n      print(\"Average loss at step\", step, \":\", average_loss)\n      average_loss = 0\n    # note that this is expensive (~20% slowdown if computed every 500 steps)\n    if step % 10000 == 0:\n      sim = similarity.eval()\n      for i in xrange(valid_size):\n        valid_word = reverse_dictionary[valid_examples[i]]\n        top_k = 8 # number of nearest neighbors\n        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n        log = \"Nearest to %s:\" % valid_word\n        for k in xrange(top_k):\n          close_word = reverse_dictionary[nearest[k]]\n          log = \"%s %s,\" % (log, close_word)\n        print(log)\n  final_embeddings = normalized_embeddings.eval()"],"metadata":{},"outputs":[],"execution_count":37}],"metadata":{"colab":{"name":"1_notmnist.ipynb","default_view":{},"version":"0.3.2","views":{},"provenance":[]},"name":"5_word2vec_tz","notebookId":250790},"nbformat":4,"nbformat_minor":0}
